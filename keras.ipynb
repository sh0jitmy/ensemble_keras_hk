import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, AveragePooling2D, GlobalAvgPool2D, Dense
from tensorflow.keras.models import Model
import tensorflow.keras.backend as K
from tensorflow.keras.callbacks import History, Callback
#from tensorflow.tpu.python.tpu import keras_support
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, AveragePooling2D, GlobalAvgPool2D, Dense
from tensorflow.keras.models import Model
import tensorflow.keras.backend as K
from tensorflow.keras.callbacks import History, Callback
#from tensorflow.contrib.tpu.python.tpu import keras_support

#from keras.objectives import categorical_crossentropy
import tensorflow.keras.losses 
#import categorical_crossentropy

import numpy as np
from sklearn.metrics import confusion_matrix, accuracy_score
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.utils import to_categorical
from scipy.stats import mode
import os, pickle

def basic_conv_block(input, chs, reps):
    x = input
    for i in range(reps):
        x =Conv2D(chs, 3, padding="same")(x)
        x = BatchNormalization()(x)
        x = Activation("relu")(x)
    return x

def create_cnn():
    input = Input(shape=(32,32,3))
    x = basic_conv_block(input, 64, 3)
    x = AveragePooling2D(2)(x)
    x = basic_conv_block(x, 128, 3)
    x = AveragePooling2D(2)(x)
    x = basic_conv_block(x, 256, 3)
    x = GlobalAvgPool2D()(x)
    x = Dense(10, activation="softmax")(x)

    model = Model(input, x)
    return model


# 確率の平均を取るアンサンブル（ソフトアンサンブル）
def ensembling_soft(models, X):
    preds_sum = None
    for model in models:
        if preds_sum is None:
            preds_sum = model.predict(X)
        else:
            preds_sum += model.predict(X)
    probs = preds_sum / len(models)
    return np.argmax(probs, axis=-1)

# 多数決のアンサンブル（ハードアンサンブル）
def ensembling_hard(models, X):
    pred_labels = np.zeros((X.shape[0], len(models)))
    for i, model in enumerate(models):
        pred_labels[:, i] = np.argmax(model.predict(X), axis=-1)
    return np.ravel(mode(pred_labels, axis=-1)[0])


class Checkpoint(Callback):
    def __init__(self, model, filepath):
        self.model = model
        self.filepath = filepath
        self.best_val_acc = 0.0

    def on_epoch_end(self, epoch, logs):
        if self.best_val_acc < logs["val_acc"]:
            self.model.save_weights(self.filepath, save_format="h5")
            self.best_val_acc = logs["val_acc"]
            print("Weights saved.", self.best_val_acc)

def train(ensemble_type):
    assert ensemble_type in ["hard", "soft"]

    (X_train, y_train), (X_test, y_test) = cifar10.load_data()
    X_train, X_test = X_train / 255.0, X_test / 255.0
    y_test_label = np.ravel(y_test)
    y_train, y_test = to_categorical(y_train), to_categorical(y_test)

    n_estimators = 5
    batch_size = 1024
    models = []
    global_hist = {"hists":[], "ensemble_test":[]}
    single_preds = np.zeros((X_test.shape[0], n_estimators))
    for i in range(n_estimators):
        print("Estimator",i+1,"train starts")
        train_model = create_cnn()
        train_model.compile(tf.keras.optimizers.Adam(), loss="categorical_crossentropy", metrics=["acc"])

        #tpu_grpc_url = "grpc://"+os.environ["COLAB_TPU_ADDR"]
        #tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu_grpc_url)
        #strategy = keras_support.TPUDistributionStrategy(tpu_cluster_resolver)
        #train_model = tf.contrib.tpu.keras_to_tpu_model(train_model, strategy=strategy)

        models.append(train_model)

        hist = History()
        cp = Checkpoint(train_model, f"weights_{i}.hdf5")
        train_model.fit(X_train, y_train, batch_size=batch_size, callbacks=[hist, cp],
                        validation_data=(X_test, y_test), epochs=100)       

        # 最良のモデルの読み込み
        train_model.load_weights(f"weights_{i}.hdf5")
        for layer in train_model.layers:
            layer.trainable = False

        # 単体の推論
        single_preds[:, i] = np.argmax(train_model.predict(X_test), axis=-1)

        # アンサンブルの精度の記録
        global_hist["hists"].append(hist.history)
        if ensemble_type == "soft":
            ensemble_test_pred = ensembling_soft(models, X_test)
        else:
            ensemble_test_pred = ensembling_hard(models, X_test)
        ensemble_test_acc = accuracy_score(y_test_label, ensemble_test_pred)

        global_hist["ensemble_test"].append(ensemble_test_acc)
        print("Current Ensemble Test Accuracy : ", ensemble_test_acc)

    global_hist["corrcoef"] = np.corrcoef(single_preds, rowvar=False)
    print("Corr Matrix on each estimators (Test)")
    print(global_hist["corrcoef"])

    with open(f"ensemble_{ensemble_type}.dat", "wb") as fp:
        pickle.dump(global_hist, fp)

if __name__ == "__main__":
    K.clear_session()
    train("soft")

from keras.objectives import categorical_crossentropy

import numpy as np
from sklearn.metrics import confusion_matrix, accuracy_score
from keras.datasets import cifar10
from keras.utils import to_categorical
from scipy.stats import mode
import os, pickle

def basic_conv_block(input, chs, reps):
    x = input
    for i in range(reps):
        x =Conv2D(chs, 3, padding="same")(x)
        x = BatchNormalization()(x)
        x = Activation("relu")(x)
    return x

def create_cnn():
    input = Input(shape=(32,32,3))
    x = basic_conv_block(input, 64, 3)
    x = AveragePooling2D(2)(x)
    x = basic_conv_block(x, 128, 3)
    x = AveragePooling2D(2)(x)
    x = basic_conv_block(x, 256, 3)
    x = GlobalAvgPool2D()(x)
    x = Dense(10, activation="softmax")(x)

    model = Model(input, x)
    return model


# 確率の平均を取るアンサンブル（ソフトアンサンブル）
def ensembling_soft(models, X):
    preds_sum = None
    for model in models:
        if preds_sum is None:
            preds_sum = model.predict(X)
        else:
            preds_sum += model.predict(X)
    probs = preds_sum / len(models)
    return np.argmax(probs, axis=-1)

# 多数決のアンサンブル（ハードアンサンブル）
def ensembling_hard(models, X):
    pred_labels = np.zeros((X.shape[0], len(models)))
    for i, model in enumerate(models):
        pred_labels[:, i] = np.argmax(model.predict(X), axis=-1)
    return np.ravel(mode(pred_labels, axis=-1)[0])


class Checkpoint(Callback):
    def __init__(self, model, filepath):
        self.model = model
        self.filepath = filepath
        self.best_val_acc = 0.0

    def on_epoch_end(self, epoch, logs):
        if self.best_val_acc < logs["val_acc"]:
            self.model.save_weights(self.filepath, save_format="h5")
            self.best_val_acc = logs["val_acc"]
            print("Weights saved.", self.best_val_acc)

def train(ensemble_type):
    assert ensemble_type in ["hard", "soft"]

    (X_train, y_train), (X_test, y_test) = cifar10.load_data()
    X_train, X_test = X_train / 255.0, X_test / 255.0
    y_test_label = np.ravel(y_test)
    y_train, y_test = to_categorical(y_train), to_categorical(y_test)

    n_estimators = 5
    batch_size = 1024
    models = []
    global_hist = {"hists":[], "ensemble_test":[]}
    single_preds = np.zeros((X_test.shape[0], n_estimators))
    for i in range(n_estimators):
        print("Estimator",i+1,"train starts")
        train_model = create_cnn()
        train_model.compile(tf.train.AdamOptimizer(), loss="categorical_crossentropy", metrics=["acc"])

        #tpu_grpc_url = "grpc://"+os.environ["COLAB_TPU_ADDR"]
        #tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu_grpc_url)
        #strategy = keras_support.TPUDistributionStrategy(tpu_cluster_resolver)
        #train_model = tf.contrib.tpu.keras_to_tpu_model(train_model, strategy=strategy)

        models.append(train_model)

        hist = History()
        cp = Checkpoint(train_model, f"weights_{i}.hdf5")
        train_model.fit(X_train, y_train, batch_size=batch_size, callbacks=[hist, cp],
                        validation_data=(X_test, y_test), epochs=100)       

        # 最良のモデルの読み込み
        train_model.load_weights(f"weights_{i}.hdf5")
        for layer in train_model.layers:
            layer.trainable = False

        # 単体の推論
        single_preds[:, i] = np.argmax(train_model.predict(X_test), axis=-1)

        # アンサンブルの精度の記録
        global_hist["hists"].append(hist.history)
        if ensemble_type == "soft":
            ensemble_test_pred = ensembling_soft(models, X_test)
        else:
            ensemble_test_pred = ensembling_hard(models, X_test)
        ensemble_test_acc = accuracy_score(y_test_label, ensemble_test_pred)

        global_hist["ensemble_test"].append(ensemble_test_acc)
        print("Current Ensemble Test Accuracy : ", ensemble_test_acc)

    global_hist["corrcoef"] = np.corrcoef(single_preds, rowvar=False)
    print("Corr Matrix on each estimators (Test)")
    print(global_hist["corrcoef"])

    with open(f"ensemble_{ensemble_type}.dat", "wb") as fp:
        pickle.dump(global_hist, fp)

if __name__ == "__main__":
    K.clear_session()
    train("soft")
